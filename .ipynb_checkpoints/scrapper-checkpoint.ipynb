{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b474c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import time\n",
    "import os\n",
    "import re as re\n",
    "import pandas as pd\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dcc123",
   "metadata": {},
   "outputs": [],
   "source": [
    "#updating local webdriver to PATH and starting driver\n",
    "try:\n",
    "    os.environ[\"PATH\"] += r\"C:/Users/yuvay/Documents/Internships/UniCult/chromedriver-win64\"\n",
    "    driver = webdriver.Chrome()\n",
    "except:\n",
    "    print(\"Location not found or chromedriver not present please verify driver path and try again\")\n",
    "    driver.quit()\n",
    "    sys.exit(\"quitting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a266eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#login details\n",
    "usr = input(\"enter linkedin email id:\")\n",
    "passw = input(\"enter linkedin password\")\n",
    "try:\n",
    "    driver.get(\"https://www.linkedin.com/uas/login\")\n",
    "    driver.implicitly_wait(5)\n",
    "\n",
    "    username = driver.find_element(By.ID, \"username\")\n",
    "    username.send_keys(usr)\n",
    "\n",
    "    psw = driver.find_element(By.ID, \"password\")\n",
    "    psw.send_keys(passw)\n",
    "\n",
    "    driver.find_element(By.XPATH, \"//button[@type='submit']\").click()\n",
    "except:\n",
    "    print(\"error logging in\")\n",
    "    driver.quit()\n",
    "    sys.exit(\"quitting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f63623e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapping code\n",
    "def scrape_posts(link, post_texts, post_names, num_posts):\n",
    "    #extracting name from the link\n",
    "    name = link.split('/')[-2]\n",
    "    print(\"Fetching data from account:\", name)\n",
    "\n",
    "    try:\n",
    "        #loading link\n",
    "        driver.get(link + 'detail/recent-activity/shares/')  \n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"occludable-update\")))\n",
    "\n",
    "        #scroll down to load all content\n",
    "        start = time.time()\n",
    "        initialScroll = 0\n",
    "        finalScroll = 1000\n",
    "        while True:\n",
    "            driver.execute_script(f\"window.scrollTo({initialScroll}, {finalScroll})\")\n",
    "            # finalScroll variable\n",
    "            initialScroll = finalScroll\n",
    "            finalScroll += 1000\n",
    "            #wait time for data to load increase if internet is slower\n",
    "            time.sleep(1)\n",
    "            end = time.time()\n",
    "            if round(end - start) > 100:\n",
    "                break\n",
    "\n",
    "        #beautifulsoup to parse the page\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        containers = soup.find_all(\"div\", {\"class\": \"occludable-update\"})\n",
    "\n",
    "        #extracting posts\n",
    "        for i, container in enumerate(containers, 1):\n",
    "            if i > num_posts:\n",
    "                break\n",
    "            text_box = container.find(\"div\", {\"class\": \"feed-shared-update-v2__description-wrapper\"})\n",
    "            text = text_box.find(\"span\", {\"dir\": \"ltr\"}).text.strip() if text_box else \"\"\n",
    "            post_texts.append(text)\n",
    "            post_names.append(name)\n",
    "\n",
    "        print(\"posts fetched:\", len(post_texts))\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while scraping {link}: {e}\")\n",
    "    finally:\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154c218f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#displaying posts\n",
    "n = int(input(\"enter the number of entries: \"))\n",
    "post_links = [input(\"enter the link: \") for _ in range(n)]\n",
    "post_texts = []\n",
    "post_names = []\n",
    "\n",
    "post_count = int(input(\"enter number of posts per account: \"))\n",
    "for link in post_links:\n",
    "    scrape_posts(link, post_texts, post_names, post_count)\n",
    "\n",
    "#printing scraped data\n",
    "for text, name in zip(post_texts, post_names):\n",
    "    print(f\"Name: {name}\\nPost: {text}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeba37d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lsa initialization for data clustering\n",
    "#initialize regex tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "#vectorize document using TF-IDF\n",
    "tfidf = TfidfVectorizer(lowercase=True,\n",
    "                        stop_words='english',\n",
    "                        ngram_range = (1,1),\n",
    "                        tokenizer = tokenizer.tokenize,\n",
    "                        token_pattern = None)\n",
    "\n",
    "#fit and transform the documents\n",
    "train_data = tfidf.fit_transform(post_texts)\n",
    "\n",
    "#define the number of topics or components\n",
    "num_components=3\n",
    "\n",
    "#create SVD object\n",
    "lsa = TruncatedSVD(n_components = num_components, n_iter = 100, random_state = 42)\n",
    "\n",
    "#fit SVD model on data\n",
    "lsa.fit_transform(train_data)\n",
    "\n",
    "#getting singular values and components \n",
    "Sigma = lsa.singular_values_ \n",
    "V_transpose = lsa.components_.T\n",
    "\n",
    "#displaying identified topics\n",
    "terms = tfidf.get_feature_names_out()\n",
    "\n",
    "topic_keywords = []\n",
    "\n",
    "for index, component in enumerate(lsa.components_):\n",
    "    zipped = zip(terms, component)\n",
    "    top_terms_key = sorted(zipped, key = lambda t: t[1], reverse=True)[:5]\n",
    "    top_terms_list = list(dict(top_terms_key).keys())\n",
    "    topic_keywords.append(top_terms_list)\n",
    "    print(\"topic \"+str(index)+\": \",top_terms_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d90824",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating best fitting topic and displaying it\n",
    "cluster_topic = []\n",
    "for post in post_texts:\n",
    "    post_split = post.split(' ')\n",
    "    topic_match_score = []\n",
    "    count = 0\n",
    "    for topic in topic_keywords:\n",
    "        for word in post_split:\n",
    "            if word in topic:\n",
    "                count += 1\n",
    "        topic_match_score.append(count)\n",
    "        count = 0\n",
    "    cluster_topic.append(topic_match_score.index(max(topic_match_score)))\n",
    "\n",
    "for post_index in range(len(post_texts)):\n",
    "    print(f\"post:{post_index+1}\\ntopic {cluster_topic[post_index]}: {topic_keywords[cluster_topic[post_index]]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
